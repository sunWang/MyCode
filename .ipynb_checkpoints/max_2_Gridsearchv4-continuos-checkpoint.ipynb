{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "            'linear' : {\n",
    "                    'mod' : LinearRegression(),\n",
    "                    'par' : {}\n",
    "                    },  \n",
    "    \n",
    "            'gradient' : {\n",
    "                    'mod' : GradientBoostingRegressor(warm_start = True),\n",
    "                    'par' : {'loss' : ('ls', 'quantile'),\n",
    "                             'max_depth' : [3, 4, 5, 6, 7]}\n",
    "                        },\n",
    "           'tree':{'mod': DecisionTreeRegressor(),\n",
    "                     'par':{'splitter':('best','random'),\n",
    "                            'max_depth': [None, 2,4,6],\n",
    "                            'min_samples_leaf':[1,5,8]}},\n",
    "            \n",
    "        'RandomForest' : {\n",
    "                    'mod' : RandomForestRegressor(n_estimators=200, random_state=39, max_depth=4, criterion = 'mse'),\n",
    "                    'par' : {'max_depth' :[None, 2,4,6],\n",
    "                            'min_samples_leaf':[1,5,8]}\n",
    "                        },\n",
    "        'Knn' : {\n",
    "                    'mod' : KNeighborsRegressor(),\n",
    "                    'par' : {'n_neighbors' :[5, 10, 15],\n",
    "                            'leaf_size':[15,25,30]}\n",
    "                        }, \n",
    "    \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(x_name,n_proc, os_X_tt, os_Y_tt, X_test, y_test,  models, score = r2_score, cv = 7):\n",
    "    \n",
    "    # Gridsearch\n",
    "    \n",
    "    result = dict()\n",
    "    bestmodels = models.copy()\n",
    "    for name in models:\n",
    "        print('*'*80)\n",
    "        print(\"Model: \" + name)\n",
    "        t_beg = time.time()\n",
    "\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), (name,  bestmodels[name]['mod'])])          \n",
    "        parameters = {}          \n",
    "        for par in bestmodels[name]['par']:\n",
    "            aux = name + '__' +  par\n",
    "            parameters[aux] = bestmodels[name]['par'][par]    \n",
    "        \n",
    "        aux = GridSearchCV(pipeline, parameters, n_jobs = n_proc,\\\n",
    "                          scoring = score, verbose=2, cv = cv)\n",
    "        \n",
    "        aux.fit(os_X_tt, os_Y_tt)\n",
    "        y_true, y_pred = y_test , aux.predict(X_test)\n",
    "        \n",
    "\n",
    "        mse = mean_squared_error(y_test,y_pred)\n",
    "        r2 = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "        \n",
    "        bestmodels[name]['bestModel'] = aux.best_estimator_\n",
    "        bestmodels[name][score] = aux.best_score_\n",
    "        bestmodels[name]['cols_order'] = os_X_tt.columns.values\n",
    "        selection_time = time.time() - t_beg\n",
    "\n",
    "        bestmodels[name]['selection_time'] = selection_time\n",
    "\n",
    "        sample_f_path = f'models/{x_name}' + f'{name}_{dt.datetime.now().strftime(\"%Y%m%d-%H%M\")}.sav'\n",
    "\n",
    "        print(f\"Saving model at {sample_f_path}\")    \n",
    "        joblib.dump(bestmodels[name]['bestModel'], sample_f_path)\n",
    "\n",
    "        print(f\"El tiempo de seleccion fue 选择时间为: {selection_time:0.3f} s\")\n",
    "        print(f\"El error {score} de la familia {name} es 错误分数: {bestmodels[name][score]:0.3f}\")\n",
    "        print('*'*80)\n",
    "    \n",
    "    \n",
    "        result[name] = {\"mse\": mse, \"r2\": r2}\n",
    "        \n",
    "    mod_name = None\n",
    "    best_mae = -np.inf\n",
    "    for name in models:\n",
    "        if bestmodels[name][score] > best_mae:\n",
    "            mod_name = name\n",
    "            best_mae = bestmodels[name][score]\n",
    "\n",
    "    print(f\"best model: \" + mod_name + f\" with an error {score} of: \" + str(best_mae))\n",
    "    \n",
    "    return bestmodels, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bestmodels\n",
    "\n",
    "def get_max(dictionary, key_val):\n",
    "    auc_list = []\n",
    "    auc_dict = {}\n",
    "\n",
    "    for key in dictionary:\n",
    "        for key2 in dictionary[key]:\n",
    "            if key_val in key2:\n",
    "                auc_list.append(dictionary[key][key_val])\n",
    "\n",
    "    max_key = ''\n",
    "    max_val = max(auc_list)\n",
    "\n",
    "    for key in dictionary:\n",
    "        for key2 in dictionary[key]:\n",
    "            if max_val == dictionary[key][key_val]:\n",
    "                max_key = key\n",
    "                \n",
    "    return max_key, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bestmodels\n",
    "\n",
    "def get_min(dictionary, key_val):\n",
    "    auc_list = []\n",
    "    auc_dict = {}\n",
    "\n",
    "    for key in dictionary:\n",
    "        for key2 in dictionary[key]:\n",
    "            if key_val in key2:\n",
    "                auc_list.append(dictionary[key][key_val])\n",
    "\n",
    "    min_key = ''\n",
    "    min_val = min(auc_list)\n",
    "\n",
    "    for key in dictionary:\n",
    "        for key2 in dictionary[key]:\n",
    "            if min_val == dictionary[key][key_val]:\n",
    "                min_key = key\n",
    "                \n",
    "    return min_key, min_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Aircraft</th>\n",
       "      <th>FlightDuration</th>\n",
       "      <th>IsInternational</th>\n",
       "      <th>SeatsEconomy</th>\n",
       "      <th>SeatsPremium</th>\n",
       "      <th>PitchEconomy</th>\n",
       "      <th>PitchPremium</th>\n",
       "      <th>WidthEconomy</th>\n",
       "      <th>WidthPremium</th>\n",
       "      <th>PriceEconomy</th>\n",
       "      <th>PricePremium</th>\n",
       "      <th>PriceRelative</th>\n",
       "      <th>SeatsTotal</th>\n",
       "      <th>PitchDifference</th>\n",
       "      <th>WidthDifference</th>\n",
       "      <th>PercentPremiumSeats</th>\n",
       "      <th>month_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.089142</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.817844</td>\n",
       "      <td>3.294215</td>\n",
       "      <td>-0.436182</td>\n",
       "      <td>-0.343385</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>-2.215259</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-1.357472</td>\n",
       "      <td>-0.578401</td>\n",
       "      <td>-0.802415</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.442612</td>\n",
       "      <td>-2.091811</td>\n",
       "      <td>-1.381970</td>\n",
       "      <td>0.009871</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.089142</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.783876</td>\n",
       "      <td>3.294215</td>\n",
       "      <td>-1.029628</td>\n",
       "      <td>-1.147585</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>-2.968588</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>-2.258698</td>\n",
       "      <td>-0.667926</td>\n",
       "      <td>-0.870953</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-1.098823</td>\n",
       "      <td>-2.650559</td>\n",
       "      <td>-1.381970</td>\n",
       "      <td>-0.411592</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.199981</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.554871</td>\n",
       "      <td>-1.878677</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-1.010589</td>\n",
       "      <td>-1.018270</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.794154</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-2.053194</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089142</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-1.542489</td>\n",
       "      <td>3.294215</td>\n",
       "      <td>-1.108754</td>\n",
       "      <td>-1.147585</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>-2.968588</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>-2.258698</td>\n",
       "      <td>-1.141274</td>\n",
       "      <td>-1.256183</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-1.169131</td>\n",
       "      <td>-2.650559</td>\n",
       "      <td>-1.381970</td>\n",
       "      <td>-0.297797</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.207793</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.161134</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.053740</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>1.630900</td>\n",
       "      <td>1.014233</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.114507</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>0.265071</td>\n",
       "      <td>-0.457953</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.767118</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-0.350788</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>1.700221</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>2.060098</td>\n",
       "      <td>0.444980</td>\n",
       "      <td>-0.792437</td>\n",
       "      <td>-0.607831</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.537737</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.988998</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.364158</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.554871</td>\n",
       "      <td>-1.878677</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-1.135100</td>\n",
       "      <td>-1.159285</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.794154</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-2.053194</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>1.608018</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>1.304591</td>\n",
       "      <td>1.557454</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.496922</td>\n",
       "      <td>0.643183</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.408838</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>0.191101</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.868795</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.056003</td>\n",
       "      <td>-1.293804</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-1.153622</td>\n",
       "      <td>-1.188433</td>\n",
       "      <td>0.74</td>\n",
       "      <td>-1.145695</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-0.637075</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>0.170051</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.251555</td>\n",
       "      <td>1.045690</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>0.553518</td>\n",
       "      <td>1.065439</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.055917</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>1.295336</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.767118</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>1.842962</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.264743</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>2.060098</td>\n",
       "      <td>0.444980</td>\n",
       "      <td>-0.495051</td>\n",
       "      <td>-0.289563</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.301996</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.261973</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.916916</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.056003</td>\n",
       "      <td>-1.293804</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-1.006473</td>\n",
       "      <td>-0.910343</td>\n",
       "      <td>1.09</td>\n",
       "      <td>-1.145695</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-0.637075</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.174505</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.056003</td>\n",
       "      <td>-1.293804</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.980747</td>\n",
       "      <td>-1.018270</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-1.145695</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-0.637075</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.089142</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.783876</td>\n",
       "      <td>3.294215</td>\n",
       "      <td>-1.029628</td>\n",
       "      <td>-1.147585</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>-2.968588</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>-2.258698</td>\n",
       "      <td>-0.938558</td>\n",
       "      <td>-1.078142</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-1.098823</td>\n",
       "      <td>-2.650559</td>\n",
       "      <td>-1.381970</td>\n",
       "      <td>-0.411592</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.222626</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.871376</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.947819</td>\n",
       "      <td>-0.985971</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.841026</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>0.509306</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.767118</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-0.350788</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>1.700221</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>2.060098</td>\n",
       "      <td>0.444980</td>\n",
       "      <td>-0.792437</td>\n",
       "      <td>-0.607831</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.537737</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.988998</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.222626</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.871376</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.834627</td>\n",
       "      <td>-0.913494</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.841026</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>0.509306</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.939562</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.056003</td>\n",
       "      <td>-1.293804</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-1.151564</td>\n",
       "      <td>-1.148256</td>\n",
       "      <td>1.04</td>\n",
       "      <td>-1.145695</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-0.637075</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.089142</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.775384</td>\n",
       "      <td>3.294215</td>\n",
       "      <td>-1.029628</td>\n",
       "      <td>-1.147585</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>-2.968588</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>-2.258698</td>\n",
       "      <td>-0.938558</td>\n",
       "      <td>-1.078142</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-1.098823</td>\n",
       "      <td>-2.650559</td>\n",
       "      <td>-1.381970</td>\n",
       "      <td>-0.411592</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.207793</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.594648</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.053740</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>2.200976</td>\n",
       "      <td>1.377405</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.114507</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>0.265071</td>\n",
       "      <td>-0.457953</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.222626</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.871376</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.947819</td>\n",
       "      <td>-0.985971</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.841026</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>0.509306</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.498406</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.082378</td>\n",
       "      <td>0.460816</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.553518</td>\n",
       "      <td>0.573857</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.887898</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>2.157229</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-1.293392</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>1.304591</td>\n",
       "      <td>1.557454</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>-1.212277</td>\n",
       "      <td>-1.300300</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.408838</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>0.191101</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>1.090010</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.080115</td>\n",
       "      <td>0.095270</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.721435</td>\n",
       "      <td>-0.123339</td>\n",
       "      <td>1.82</td>\n",
       "      <td>-0.055917</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>0.119452</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.717989</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>1.304591</td>\n",
       "      <td>1.557454</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>-1.245205</td>\n",
       "      <td>-1.331024</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.408838</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>0.191101</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.207793</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>0.192696</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.752686</td>\n",
       "      <td>-0.928258</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>1.582536</td>\n",
       "      <td>0.903154</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.817590</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.411592</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.852236</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>1.543434</td>\n",
       "      <td>1.365588</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.483113</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.327300</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-0.231901</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.381453</td>\n",
       "      <td>0.314598</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>0.206739</td>\n",
       "      <td>0.962239</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.389369</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>-0.091280</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.710704</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>1.112275</td>\n",
       "      <td>1.122160</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.483113</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.327300</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>0.404994</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.251555</td>\n",
       "      <td>1.045690</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>0.799454</td>\n",
       "      <td>0.851160</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.055917</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>1.295336</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.174505</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.056003</td>\n",
       "      <td>-1.293804</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.700855</td>\n",
       "      <td>-0.850471</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-1.145695</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-0.637075</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.042247</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.367266</td>\n",
       "      <td>0.395816</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.483113</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.327300</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>0.099285</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.251555</td>\n",
       "      <td>1.045690</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.756422</td>\n",
       "      <td>-0.930825</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.055917</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>1.295336</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1.767118</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.010328</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.264743</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>2.060098</td>\n",
       "      <td>0.444980</td>\n",
       "      <td>-0.732754</td>\n",
       "      <td>-0.911131</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.301996</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.261973</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.382349</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.082378</td>\n",
       "      <td>0.460816</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.274655</td>\n",
       "      <td>0.242985</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.887898</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>2.157229</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.710704</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>2.254103</td>\n",
       "      <td>2.361654</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>0.520590</td>\n",
       "      <td>1.365588</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.381436</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>0.108915</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1.767118</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>1.514607</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.264743</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>2.060098</td>\n",
       "      <td>0.444980</td>\n",
       "      <td>0.160434</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.301996</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.261973</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.492320</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.554871</td>\n",
       "      <td>-1.878677</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-1.119665</td>\n",
       "      <td>-1.060023</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-0.794154</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-2.053194</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.948478</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>2.254103</td>\n",
       "      <td>2.361654</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>1.203858</td>\n",
       "      <td>1.511330</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2.381436</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>0.108915</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.492320</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.554871</td>\n",
       "      <td>-1.878677</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-1.119665</td>\n",
       "      <td>-1.060023</td>\n",
       "      <td>1.30</td>\n",
       "      <td>-0.794154</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-2.053194</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-1.010328</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.082378</td>\n",
       "      <td>0.460816</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>-0.052573</td>\n",
       "      <td>-0.207632</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.887898</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>2.157229</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-0.138489</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.381453</td>\n",
       "      <td>0.314598</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>0.163521</td>\n",
       "      <td>0.950422</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.389369</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>-0.091280</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>1.767118</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>1.842962</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.264743</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>2.060098</td>\n",
       "      <td>0.444980</td>\n",
       "      <td>-0.495051</td>\n",
       "      <td>-0.082373</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-0.301996</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.261973</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.648468</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.916916</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.056003</td>\n",
       "      <td>-1.293804</td>\n",
       "      <td>-1.818641</td>\n",
       "      <td>1.551387</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.978689</td>\n",
       "      <td>-0.910343</td>\n",
       "      <td>0.91</td>\n",
       "      <td>-1.145695</td>\n",
       "      <td>1.819422</td>\n",
       "      <td>1.912112</td>\n",
       "      <td>-0.637075</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.051164</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>1.630563</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.563809</td>\n",
       "      <td>0.608520</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.717474</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>0.901267</td>\n",
       "      <td>-1.639972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-0.090368</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.251555</td>\n",
       "      <td>1.045690</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>0.662594</td>\n",
       "      <td>0.499017</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.055917</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>1.295336</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.051164</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>1.630563</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.564838</td>\n",
       "      <td>0.609308</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.717474</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>0.901267</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.089142</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>-0.888610</td>\n",
       "      <td>3.294215</td>\n",
       "      <td>-1.029628</td>\n",
       "      <td>-1.147585</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>-2.968588</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>-2.258698</td>\n",
       "      <td>-0.974573</td>\n",
       "      <td>-1.113593</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-1.098823</td>\n",
       "      <td>-2.650559</td>\n",
       "      <td>-1.381970</td>\n",
       "      <td>-0.411592</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>1.019244</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.982619</td>\n",
       "      <td>1.143431</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.483113</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.327300</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>1.041889</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>-0.259406</td>\n",
       "      <td>-0.454211</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.483113</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.327300</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1.207793</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>0.594648</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.752686</td>\n",
       "      <td>-0.928258</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>-0.663810</td>\n",
       "      <td>-0.129641</td>\n",
       "      <td>1.56</td>\n",
       "      <td>-0.817590</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.411592</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>1.160776</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.082378</td>\n",
       "      <td>0.460816</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.442385</td>\n",
       "      <td>0.955936</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.887898</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>2.157229</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>1.207793</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.569172</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.053740</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>-1.455802</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>2.268892</td>\n",
       "      <td>1.428611</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.114507</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>0.265071</td>\n",
       "      <td>-0.457953</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-0.231901</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.381453</td>\n",
       "      <td>0.314598</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>0.206739</td>\n",
       "      <td>0.962239</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.389369</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>-0.091280</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.334228</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.513330</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>0.295235</td>\n",
       "      <td>1.042593</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.483113</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.327300</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1.767118</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.971123</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.264743</td>\n",
       "      <td>-0.416494</td>\n",
       "      <td>1.220342</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>2.060098</td>\n",
       "      <td>0.444980</td>\n",
       "      <td>-0.685419</td>\n",
       "      <td>-0.689761</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.301996</td>\n",
       "      <td>-0.415569</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>-0.261973</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-0.138489</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>0.381453</td>\n",
       "      <td>0.314598</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>0.163521</td>\n",
       "      <td>0.950422</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.389369</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>-0.091280</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>1.441153</td>\n",
       "      <td>-1.174505</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>1.304591</td>\n",
       "      <td>1.557454</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>-0.945761</td>\n",
       "      <td>-1.086808</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.408838</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>0.191101</td>\n",
       "      <td>0.282340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-1.029508</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>0.240817</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-1.082378</td>\n",
       "      <td>0.460816</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>-0.456246</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>-0.443182</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.887898</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>-0.558450</td>\n",
       "      <td>2.157229</td>\n",
       "      <td>1.243496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-0.470183</td>\n",
       "      <td>-0.693889</td>\n",
       "      <td>1.466486</td>\n",
       "      <td>-0.303562</td>\n",
       "      <td>-0.080115</td>\n",
       "      <td>0.095270</td>\n",
       "      <td>-0.299150</td>\n",
       "      <td>0.044729</td>\n",
       "      <td>0.302148</td>\n",
       "      <td>1.346207</td>\n",
       "      <td>-0.184287</td>\n",
       "      <td>-0.051650</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.055917</td>\n",
       "      <td>0.143179</td>\n",
       "      <td>1.088591</td>\n",
       "      <td>0.119452</td>\n",
       "      <td>-0.678816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Airline  Aircraft  FlightDuration  IsInternational  SeatsEconomy  \\\n",
       "0    0.089142 -0.693889       -0.817844         3.294215     -0.436182   \n",
       "1    0.089142 -0.693889       -0.783876         3.294215     -1.029628   \n",
       "2    0.648468 -0.693889       -1.199981        -0.303562     -0.554871   \n",
       "3    0.089142  1.441153       -1.542489         3.294215     -1.108754   \n",
       "4    1.207793 -0.693889       -0.161134        -0.303562     -0.053740   \n",
       "5    1.767118  1.441153       -0.350788        -0.303562      1.700221   \n",
       "6    0.648468 -0.693889       -1.364158        -0.303562     -0.554871   \n",
       "7   -1.029508  1.441153        1.608018        -0.303562      1.304591   \n",
       "8    0.648468 -0.693889       -0.868795        -0.303562     -1.056003   \n",
       "9   -0.470183  1.441153        0.170051        -0.303562     -0.251555   \n",
       "10   1.767118 -0.693889        1.842962        -0.303562     -0.264743   \n",
       "11   0.648468 -0.693889       -0.916916        -0.303562     -1.056003   \n",
       "12   0.648468 -0.693889       -1.174505        -0.303562     -1.056003   \n",
       "13   0.089142 -0.693889       -0.783876         3.294215     -1.029628   \n",
       "14   0.648468 -0.693889       -1.222626        -0.303562     -0.871376   \n",
       "15   1.767118  1.441153       -0.350788        -0.303562      1.700221   \n",
       "16   0.648468 -0.693889       -1.222626        -0.303562     -0.871376   \n",
       "17   0.648468 -0.693889       -0.939562        -0.303562     -1.056003   \n",
       "18   0.089142 -0.693889       -0.775384         3.294215     -1.029628   \n",
       "19   1.207793 -0.693889        0.594648        -0.303562     -0.053740   \n",
       "20   0.648468 -0.693889       -1.222626        -0.303562     -0.871376   \n",
       "21  -1.029508 -0.693889        0.498406        -0.303562     -1.082378   \n",
       "22  -1.029508  1.441153       -1.293392        -0.303562      1.304591   \n",
       "23  -0.470183 -0.693889        1.090010        -0.303562     -0.080115   \n",
       "24  -1.029508 -0.693889       -1.717989        -0.303562      1.304591   \n",
       "25   1.207793  1.441153        0.192696        -0.303562     -0.752686   \n",
       "26  -1.029508 -0.693889        0.852236        -0.303562      0.513330   \n",
       "27  -0.470183  1.441153       -0.231901        -0.303562      0.381453   \n",
       "28  -1.029508 -0.693889        0.710704        -0.303562      0.513330   \n",
       "29  -0.470183  1.441153        0.404994        -0.303562     -0.251555   \n",
       "..        ...       ...             ...              ...           ...   \n",
       "290  0.648468 -0.693889       -1.174505        -0.303562     -1.056003   \n",
       "291 -1.029508 -0.693889       -0.042247        -0.303562      0.513330   \n",
       "292 -0.470183  1.441153        0.099285        -0.303562     -0.251555   \n",
       "293  1.767118 -0.693889       -1.010328        -0.303562     -0.264743   \n",
       "294 -1.029508 -0.693889        0.382349        -0.303562     -1.082378   \n",
       "295 -0.470183 -0.693889        0.710704        -0.303562      2.254103   \n",
       "296  1.767118 -0.693889        1.514607        -0.303562     -0.264743   \n",
       "297  0.648468 -0.693889       -0.492320        -0.303562     -0.554871   \n",
       "298 -0.470183 -0.693889        0.948478        -0.303562      2.254103   \n",
       "299  0.648468 -0.693889       -0.492320        -0.303562     -0.554871   \n",
       "300 -1.029508 -0.693889       -1.010328        -0.303562     -1.082378   \n",
       "301 -0.470183  1.441153       -0.138489        -0.303562      0.381453   \n",
       "302  1.767118 -0.693889        1.842962        -0.303562     -0.264743   \n",
       "303  0.648468 -0.693889       -0.916916        -0.303562     -1.056003   \n",
       "304 -1.029508 -0.693889        0.051164        -0.303562      0.513330   \n",
       "305 -0.470183  1.441153       -0.090368        -0.303562     -0.251555   \n",
       "306 -1.029508 -0.693889        0.051164        -0.303562      0.513330   \n",
       "307  0.089142 -0.693889       -0.888610         3.294215     -1.029628   \n",
       "308 -1.029508 -0.693889        1.019244        -0.303562      0.513330   \n",
       "309 -1.029508 -0.693889        1.041889        -0.303562      0.513330   \n",
       "310  1.207793  1.441153        0.594648        -0.303562     -0.752686   \n",
       "311 -1.029508 -0.693889        1.160776        -0.303562     -1.082378   \n",
       "312  1.207793 -0.693889        0.569172        -0.303562     -0.053740   \n",
       "313 -0.470183  1.441153       -0.231901        -0.303562      0.381453   \n",
       "314 -1.029508 -0.693889        0.334228        -0.303562      0.513330   \n",
       "315  1.767118 -0.693889        0.971123        -0.303562     -0.264743   \n",
       "316 -0.470183  1.441153       -0.138489        -0.303562      0.381453   \n",
       "317 -1.029508  1.441153       -1.174505        -0.303562      1.304591   \n",
       "318 -1.029508 -0.693889        0.240817        -0.303562     -1.082378   \n",
       "319 -0.470183 -0.693889        1.466486        -0.303562     -0.080115   \n",
       "\n",
       "     SeatsPremium  PitchEconomy  PitchPremium  WidthEconomy  WidthPremium  \\\n",
       "0       -0.343385      1.220342     -2.215259      0.302148     -1.357472   \n",
       "1       -1.147585      1.220342     -2.968588     -1.455802     -2.258698   \n",
       "2       -1.878677     -1.818641      1.551387     -1.455802      1.346207   \n",
       "3       -1.147585      1.220342     -2.968588     -1.455802     -2.258698   \n",
       "4       -0.416494      1.220342      0.044729     -1.455802     -0.456246   \n",
       "5        0.168380      1.220342      0.044729      2.060098      0.444980   \n",
       "6       -1.878677     -1.818641      1.551387     -1.455802      1.346207   \n",
       "7        1.557454     -0.299150      0.044729      0.302148     -0.456246   \n",
       "8       -1.293804     -1.818641      1.551387     -1.455802      1.346207   \n",
       "9        1.045690     -0.299150      0.044729      0.302148      1.346207   \n",
       "10      -0.416494      1.220342      0.044729      2.060098      0.444980   \n",
       "11      -1.293804     -1.818641      1.551387     -1.455802      1.346207   \n",
       "12      -1.293804     -1.818641      1.551387     -1.455802      1.346207   \n",
       "13      -1.147585      1.220342     -2.968588     -1.455802     -2.258698   \n",
       "14      -0.416494     -1.818641      1.551387     -1.455802      1.346207   \n",
       "15       0.168380      1.220342      0.044729      2.060098      0.444980   \n",
       "16      -0.416494     -1.818641      1.551387     -1.455802      1.346207   \n",
       "17      -1.293804     -1.818641      1.551387     -1.455802      1.346207   \n",
       "18      -1.147585      1.220342     -2.968588     -1.455802     -2.258698   \n",
       "19      -0.416494      1.220342      0.044729     -1.455802     -0.456246   \n",
       "20      -0.416494     -1.818641      1.551387     -1.455802      1.346207   \n",
       "21       0.460816     -0.299150      0.044729      0.302148     -0.456246   \n",
       "22       1.557454     -0.299150      0.044729      0.302148     -0.456246   \n",
       "23       0.095270     -0.299150      0.044729      0.302148      1.346207   \n",
       "24       1.557454     -0.299150      0.044729      0.302148     -0.456246   \n",
       "25      -0.928258      1.220342      0.044729      0.302148     -0.456246   \n",
       "26       0.168380     -0.299150      0.044729      0.302148     -0.456246   \n",
       "27       0.314598     -0.299150      0.044729      0.302148      1.346207   \n",
       "28       0.168380     -0.299150      0.044729      0.302148     -0.456246   \n",
       "29       1.045690     -0.299150      0.044729      0.302148      1.346207   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "290     -1.293804     -1.818641      1.551387     -1.455802      1.346207   \n",
       "291      0.168380     -0.299150      0.044729      0.302148     -0.456246   \n",
       "292      1.045690     -0.299150      0.044729      0.302148      1.346207   \n",
       "293     -0.416494      1.220342      0.044729      2.060098      0.444980   \n",
       "294      0.460816     -0.299150      0.044729      0.302148     -0.456246   \n",
       "295      2.361654     -0.299150      0.044729      0.302148      1.346207   \n",
       "296     -0.416494      1.220342      0.044729      2.060098      0.444980   \n",
       "297     -1.878677     -1.818641      1.551387     -1.455802      1.346207   \n",
       "298      2.361654     -0.299150      0.044729      0.302148      1.346207   \n",
       "299     -1.878677     -1.818641      1.551387     -1.455802      1.346207   \n",
       "300      0.460816     -0.299150      0.044729      0.302148     -0.456246   \n",
       "301      0.314598     -0.299150      0.044729      0.302148      1.346207   \n",
       "302     -0.416494      1.220342      0.044729      2.060098      0.444980   \n",
       "303     -1.293804     -1.818641      1.551387     -1.455802      1.346207   \n",
       "304      1.630563     -0.299150      0.044729      0.302148     -0.456246   \n",
       "305      1.045690     -0.299150      0.044729      0.302148      1.346207   \n",
       "306      1.630563     -0.299150      0.044729      0.302148     -0.456246   \n",
       "307     -1.147585      1.220342     -2.968588     -1.455802     -2.258698   \n",
       "308      0.168380     -0.299150      0.044729      0.302148     -0.456246   \n",
       "309      0.168380     -0.299150      0.044729      0.302148     -0.456246   \n",
       "310     -0.928258      1.220342      0.044729      0.302148     -0.456246   \n",
       "311      0.460816     -0.299150      0.044729      0.302148     -0.456246   \n",
       "312     -0.416494      1.220342      0.044729     -1.455802     -0.456246   \n",
       "313      0.314598     -0.299150      0.044729      0.302148      1.346207   \n",
       "314      0.168380     -0.299150      0.044729      0.302148     -0.456246   \n",
       "315     -0.416494      1.220342      0.044729      2.060098      0.444980   \n",
       "316      0.314598     -0.299150      0.044729      0.302148      1.346207   \n",
       "317      1.557454     -0.299150      0.044729      0.302148     -0.456246   \n",
       "318      0.460816     -0.299150      0.044729      0.302148     -0.456246   \n",
       "319      0.095270     -0.299150      0.044729      0.302148      1.346207   \n",
       "\n",
       "     PriceEconomy  PricePremium  PriceRelative  SeatsTotal  PitchDifference  \\\n",
       "0       -0.578401     -0.802415           0.06   -0.442612        -2.091811   \n",
       "1       -0.667926     -0.870953           0.07   -1.098823        -2.650559   \n",
       "2       -1.010589     -1.018270           0.65   -0.794154         1.819422   \n",
       "3       -1.141274     -1.256183           0.09   -1.169131        -2.650559   \n",
       "4        1.630900      1.014233           0.07   -0.114507        -0.415569   \n",
       "5       -0.792437     -0.607831           0.99    1.537737        -0.415569   \n",
       "6       -1.135100     -1.159285           0.77   -0.794154         1.819422   \n",
       "7        0.496922      0.643183           0.47    1.408838         0.143179   \n",
       "8       -1.153622     -1.188433           0.74   -1.145695         1.819422   \n",
       "9        0.553518      1.065439           0.73   -0.055917         0.143179   \n",
       "10      -0.495051     -0.289563           0.77   -0.301996        -0.415569   \n",
       "11      -1.006473     -0.910343           1.09   -1.145695         1.819422   \n",
       "12      -0.980747     -1.018270           0.50   -1.145695         1.819422   \n",
       "13      -0.938558     -1.078142           0.12   -1.098823        -2.650559   \n",
       "14      -0.947819     -0.985971           0.48   -0.841026         1.819422   \n",
       "15      -0.792437     -0.607831           0.99    1.537737        -0.415569   \n",
       "16      -0.834627     -0.913494           0.33   -0.841026         1.819422   \n",
       "17      -1.151564     -1.148256           1.04   -1.145695         1.819422   \n",
       "18      -0.938558     -1.078142           0.12   -1.098823        -2.650559   \n",
       "19       2.200976      1.377405           0.03   -0.114507        -0.415569   \n",
       "20      -0.947819     -0.985971           0.48   -0.841026         1.819422   \n",
       "21       0.553518      0.573857           0.38   -0.887898         0.143179   \n",
       "22      -1.212277     -1.300300           0.29    1.408838         0.143179   \n",
       "23      -0.721435     -0.123339           1.82   -0.055917         0.143179   \n",
       "24      -1.245205     -1.331024           0.33    1.408838         0.143179   \n",
       "25       1.582536      0.903154           0.04   -0.817590        -0.415569   \n",
       "26       1.543434      1.365588           0.26    0.483113         0.143179   \n",
       "27       0.206739      0.962239           1.03    0.389369         0.143179   \n",
       "28       1.112275      1.122160           0.36    0.483113         0.143179   \n",
       "29       0.799454      0.851160           0.39   -0.055917         0.143179   \n",
       "..            ...           ...            ...         ...              ...   \n",
       "290     -0.700855     -0.850471           0.17   -1.145695         1.819422   \n",
       "291      0.367266      0.395816           0.40    0.483113         0.143179   \n",
       "292     -0.756422     -0.930825           0.10   -0.055917         0.143179   \n",
       "293     -0.732754     -0.911131           0.10   -0.301996        -0.415569   \n",
       "294      0.274655      0.242985           0.35   -0.887898         0.143179   \n",
       "295      0.520590      1.365588           0.97    2.381436         0.143179   \n",
       "296      0.160434     -0.166667           0.09   -0.301996        -0.415569   \n",
       "297     -1.119665     -1.060023           1.30   -0.794154         1.819422   \n",
       "298      1.203858      1.511330           0.51    2.381436         0.143179   \n",
       "299     -1.119665     -1.060023           1.30   -0.794154         1.819422   \n",
       "300     -0.052573     -0.207632           0.24   -0.887898         0.143179   \n",
       "301      0.163521      0.950422           1.08    0.389369         0.143179   \n",
       "302     -0.495051     -0.082373           1.11   -0.301996        -0.415569   \n",
       "303     -0.978689     -0.910343           0.91   -1.145695         1.819422   \n",
       "304      0.563809      0.608520           0.40    0.717474         0.143179   \n",
       "305      0.662594      0.499017           0.26   -0.055917         0.143179   \n",
       "306      0.564838      0.609308           0.40    0.717474         0.143179   \n",
       "307     -0.974573     -1.113593           0.10   -1.098823        -2.650559   \n",
       "308      0.982619      1.143431           0.45    0.483113         0.143179   \n",
       "309     -0.259406     -0.454211           0.17    0.483113         0.143179   \n",
       "310     -0.663810     -0.129641           1.56   -0.817590        -0.415569   \n",
       "311      0.442385      0.955936           0.75   -0.887898         0.143179   \n",
       "312      2.268892      1.428611           0.03   -0.114507        -0.415569   \n",
       "313      0.206739      0.962239           1.03    0.389369         0.143179   \n",
       "314      0.295235      1.042593           0.98    0.483113         0.143179   \n",
       "315     -0.685419     -0.689761           0.48   -0.301996        -0.415569   \n",
       "316      0.163521      0.950422           1.08    0.389369         0.143179   \n",
       "317     -0.945761     -1.086808           0.11    1.408838         0.143179   \n",
       "318     -0.156504     -0.443182           0.08   -0.887898         0.143179   \n",
       "319     -0.184287     -0.051650           0.56   -0.055917         0.143179   \n",
       "\n",
       "     WidthDifference  PercentPremiumSeats  month_num  \n",
       "0          -1.381970             0.009871  -1.639972  \n",
       "1          -1.381970            -0.411592  -1.639972  \n",
       "2           1.912112            -2.053194   0.282340  \n",
       "3          -1.381970            -0.297797   0.282340  \n",
       "4           0.265071            -0.457953  -0.678816  \n",
       "5          -0.558450            -0.988998  -0.678816  \n",
       "6           1.912112            -2.053194   1.243496  \n",
       "7          -0.558450             0.191101  -0.678816  \n",
       "8           1.912112            -0.637075   1.243496  \n",
       "9           1.088591             1.295336   0.282340  \n",
       "10         -0.558450            -0.261973  -1.639972  \n",
       "11          1.912112            -0.637075  -0.678816  \n",
       "12          1.912112            -0.637075   0.282340  \n",
       "13         -1.381970            -0.411592  -0.678816  \n",
       "14          1.912112             0.509306  -1.639972  \n",
       "15         -0.558450            -0.988998   0.282340  \n",
       "16          1.912112             0.509306  -0.678816  \n",
       "17          1.912112            -0.637075   0.282340  \n",
       "18         -1.381970            -0.411592   1.243496  \n",
       "19          0.265071            -0.457953   1.243496  \n",
       "20          1.912112             0.509306   1.243496  \n",
       "21         -0.558450             2.157229   1.243496  \n",
       "22         -0.558450             0.191101   1.243496  \n",
       "23          1.088591             0.119452   1.243496  \n",
       "24         -0.558450             0.191101   1.243496  \n",
       "25         -0.558450            -0.411592   0.282340  \n",
       "26         -0.558450            -0.327300   0.282340  \n",
       "27          1.088591            -0.091280  -1.639972  \n",
       "28         -0.558450            -0.327300  -1.639972  \n",
       "29          1.088591             1.295336  -0.678816  \n",
       "..               ...                  ...        ...  \n",
       "290         1.912112            -0.637075  -1.639972  \n",
       "291        -0.558450            -0.327300   1.243496  \n",
       "292         1.088591             1.295336  -0.678816  \n",
       "293        -0.558450            -0.261973   1.243496  \n",
       "294        -0.558450             2.157229  -0.678816  \n",
       "295         1.088591             0.108915   0.282340  \n",
       "296        -0.558450            -0.261973  -1.639972  \n",
       "297         1.912112            -2.053194  -1.639972  \n",
       "298         1.088591             0.108915  -0.678816  \n",
       "299         1.912112            -2.053194   0.282340  \n",
       "300        -0.558450             2.157229  -0.678816  \n",
       "301         1.088591            -0.091280   1.243496  \n",
       "302        -0.558450            -0.261973  -0.678816  \n",
       "303         1.912112            -0.637075   0.282340  \n",
       "304        -0.558450             0.901267  -1.639972  \n",
       "305         1.088591             1.295336   0.282340  \n",
       "306        -0.558450             0.901267   1.243496  \n",
       "307        -1.381970            -0.411592  -0.678816  \n",
       "308        -0.558450            -0.327300  -0.678816  \n",
       "309        -0.558450            -0.327300   0.282340  \n",
       "310        -0.558450            -0.411592   1.243496  \n",
       "311        -0.558450             2.157229   0.282340  \n",
       "312         0.265071            -0.457953   1.243496  \n",
       "313         1.088591            -0.091280  -0.678816  \n",
       "314        -0.558450            -0.327300   1.243496  \n",
       "315        -0.558450            -0.261973   0.282340  \n",
       "316         1.088591            -0.091280   0.282340  \n",
       "317        -0.558450             0.191101   0.282340  \n",
       "318        -0.558450             2.157229   1.243496  \n",
       "319         1.088591             0.119452  -0.678816  \n",
       "\n",
       "[320 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'max_data/SixAirlinesDataV2.csv'\n",
    "data = pd.read_csv(path, sep = ',', na_filter = False)\n",
    "#data = data.set_index('id')\n",
    "data=data.drop(columns='Unnamed: 0')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables seleccionadas según diferentes criterios  根据不同标准选择变量\n",
    "y = data[['PriceEconomy']].copy()\n",
    "X = data.drop(columns = ['PriceEconomy']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del tamaño del test  测试尺寸定义\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables base original  原始基本变量\n",
    "X0_train= X_train\n",
    "X0_test= X_test\n",
    "\n",
    "# variables con probabilidad de selección mayor al 70%  选择概率大于70％的变量\n",
    "X1_train= X_train[['Airline','Aircraft','FlightDuration','IsInternational','SeatsEconomy','SeatsPremium','PitchEconomy','WidthEconomy','WidthPremium','PriceRelative','PricePremium','PercentPremiumSeats','month_num']].copy()\n",
    "X1_test = X_test[['Airline','Aircraft','FlightDuration','IsInternational','SeatsEconomy','SeatsPremium','PitchEconomy','WidthEconomy','WidthPremium','PriceRelative','PricePremium','PercentPremiumSeats','month_num']].copy()\n",
    "\n",
    "# variables con probabilidad de selección mayor al 80%\n",
    "X2_train= X_train[['Airline','Aircraft','FlightDuration','IsInternational','SeatsEconomy','SeatsPremium','PitchEconomy','WidthEconomy','WidthPremium','PriceRelative','PricePremium','PercentPremiumSeats']].copy()\n",
    "X2_test= X_test[['Airline','Aircraft','FlightDuration','IsInternational','SeatsEconomy','SeatsPremium','PitchEconomy','WidthEconomy','WidthPremium','PriceRelative','PricePremium','PercentPremiumSeats']].copy()\n",
    "\n",
    "# variables con probabilidad de selección mayor al 90%\n",
    "X3_train= X_train[['FlightDuration','IsInternational','SeatsEconomy','SeatsPremium','PitchEconomy','PriceRelative','PricePremium','month_num']].copy()\n",
    "X3_test= X_test[['FlightDuration','IsInternational','SeatsEconomy','SeatsPremium','PitchEconomy','PriceRelative','PricePremium','month_num']].copy()\n",
    "\n",
    "# variables con probabilidad de selección mayor al 100%\n",
    "X4_train= X_train[['FlightDuration','PricePremium','PriceRelative']].copy()\n",
    "X4_test= X_test[['FlightDuration','PricePremium','PriceRelative']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Model: linear\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1linear_20191126-1923.sav\n",
      "El tiempo de seleccion fue 选择时间为: 5.734 s\n",
      "El error r2 de la familia linear es 错误分数: 0.943\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: gradient\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    7.9s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1gradient_20191126-1923.sav\n",
      "El tiempo de seleccion fue 选择时间为: 8.555 s\n",
      "El error r2 de la familia gradient es 错误分数: 0.996\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: tree\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    4.6s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1tree_20191126-1923.sav\n",
      "El tiempo de seleccion fue 选择时间为: 5.095 s\n",
      "El error r2 de la familia tree es 错误分数: 0.993\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: RandomForest\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   12.0s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:356: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1RandomForest_20191126-1924.sav\n",
      "El tiempo de seleccion fue 选择时间为: 13.002 s\n",
      "El error r2 de la familia RandomForest es 错误分数: 0.990\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: Knn\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1Knn_20191126-1924.sav\n",
      "El tiempo de seleccion fue 选择时间为: 5.594 s\n",
      "El error r2 de la familia Knn es 错误分数: 0.821\n",
      "********************************************************************************\n",
      "best model: gradient with an error r2 of: 0.9963446175658611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  45 | elapsed:    5.0s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    5.1s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Bestmodels_X0, result_X0  = grid('X1', -1, X0_train, y_train.values, X0_test, y_test, models, score = 'r2', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': {'mse': 0.03567231840114411, 'r2': 0.9667099963998388},\n",
       " 'gradient': {'mse': 0.02261103368823994, 'r2': 0.9788990055420471},\n",
       " 'tree': {'mse': 0.060210876785160995, 'r2': 0.943810203687726},\n",
       " 'RandomForest': {'mse': 0.01698231302892388, 'r2': 0.9841518217146826},\n",
       " 'Knn': {'mse': 0.15330473650832402, 'r2': 0.8569334582380217}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Model: linear\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1linear_20191126-1924.sav\n",
      "El tiempo de seleccion fue 选择时间为: 4.201 s\n",
      "El error r2 de la familia linear es 错误分数: 0.944\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: gradient\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    7.3s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1gradient_20191126-1924.sav\n",
      "El tiempo de seleccion fue 选择时间为: 7.776 s\n",
      "El error r2 de la familia gradient es 错误分数: 0.996\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: tree\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    4.8s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1tree_20191126-1924.sav\n",
      "El tiempo de seleccion fue 选择时间为: 5.324 s\n",
      "El error r2 de la familia tree es 错误分数: 0.992\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: RandomForest\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   15.2s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:356: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1RandomForest_20191126-1924.sav\n",
      "El tiempo de seleccion fue 选择时间为: 15.996 s\n",
      "El error r2 de la familia RandomForest es 错误分数: 0.990\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: Knn\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X1Knn_20191126-1924.sav\n",
      "El tiempo de seleccion fue 选择时间为: 4.583 s\n",
      "El error r2 de la familia Knn es 错误分数: 0.840\n",
      "********************************************************************************\n",
      "best model: gradient with an error r2 of: 0.9963728143137898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  45 | elapsed:    4.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    4.2s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "Bestmodels_X1, result_X1  = grid('X1', -1, X1_train, y_train.values, X1_test, y_test, models, score = 'r2', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': {'mse': 0.03557579752335649, 'r2': 0.9668000712958101},\n",
       " 'gradient': {'mse': 0.022581152949204625, 'r2': 0.9789268907470083},\n",
       " 'tree': {'mse': 0.06095075684825718, 'r2': 0.9431197352497853},\n",
       " 'RandomForest': {'mse': 0.01624914044974528, 'r2': 0.9848360306165523},\n",
       " 'Knn': {'mse': 0.12292926080913003, 'r2': 0.8852803597208909}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Model: linear\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X2linear_20191126-1925.sav\n",
      "El tiempo de seleccion fue 选择时间为: 4.932 s\n",
      "El error r2 de la familia linear es 错误分数: 0.945\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: gradient\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    9.3s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X2gradient_20191126-1925.sav\n",
      "El tiempo de seleccion fue 选择时间为: 9.803 s\n",
      "El error r2 de la familia gradient es 错误分数: 0.996\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: tree\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    6.5s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X2tree_20191126-1925.sav\n",
      "El tiempo de seleccion fue 选择时间为: 6.976 s\n",
      "El error r2 de la familia tree es 错误分数: 0.993\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: RandomForest\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   21.0s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:356: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X2RandomForest_20191126-1925.sav\n",
      "El tiempo de seleccion fue 选择时间为: 21.837 s\n",
      "El error r2 de la familia RandomForest es 错误分数: 0.990\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: Knn\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    6.6s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X2Knn_20191126-1925.sav\n",
      "El tiempo de seleccion fue 选择时间为: 7.080 s\n",
      "El error r2 de la familia Knn es 错误分数: 0.859\n",
      "********************************************************************************\n",
      "best model: gradient with an error r2 of: 0.9964606780046588\n"
     ]
    }
   ],
   "source": [
    "Bestmodels_X2, result_X2  = grid('X2', -1, X2_train, y_train.values, X2_test, y_test, models, score = 'r2', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': {'mse': 0.034795150990073444, 'r2': 0.9675285836849183},\n",
       " 'gradient': {'mse': 0.022484600767427147, 'r2': 0.9790169948563862},\n",
       " 'tree': {'mse': 0.05978239513201922, 'r2': 0.9442100699261714},\n",
       " 'RandomForest': {'mse': 0.016616572125573312, 'r2': 0.9844931372370535},\n",
       " 'Knn': {'mse': 0.1000875812312455, 'r2': 0.9065965967770481}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Model: linear\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X3linear_20191126-1925.sav\n",
      "El tiempo de seleccion fue 选择时间为: 5.421 s\n",
      "El error r2 de la familia linear es 错误分数: 0.929\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: gradient\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    9.2s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X3gradient_20191126-1926.sav\n",
      "El tiempo de seleccion fue 选择时间为: 9.689 s\n",
      "El error r2 de la familia gradient es 错误分数: 0.997\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: tree\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    6.4s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X3tree_20191126-1926.sav\n",
      "El tiempo de seleccion fue 选择时间为: 6.892 s\n",
      "El error r2 de la familia tree es 错误分数: 0.993\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: RandomForest\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   21.2s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:356: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X3RandomForest_20191126-1926.sav\n",
      "El tiempo de seleccion fue 选择时间为: 22.575 s\n",
      "El error r2 de la familia RandomForest es 错误分数: 0.992\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: Knn\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X3Knn_20191126-1926.sav\n",
      "El tiempo de seleccion fue 选择时间为: 7.576 s\n",
      "El error r2 de la familia Knn es 错误分数: 0.882\n",
      "********************************************************************************\n",
      "best model: gradient with an error r2 of: 0.997330240102161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    7.0s finished\n"
     ]
    }
   ],
   "source": [
    "Bestmodels_X3, result_X3  = grid('X3', -1, X3_train, y_train.values, X3_test, y_test, models, score = 'r2', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': {'mse': 0.04644195394196985, 'r2': 0.9566595925574293},\n",
       " 'gradient': {'mse': 0.003713039034189975, 'r2': 0.9965349299301006},\n",
       " 'tree': {'mse': 0.0125163903653258, 'r2': 0.9883194953678885},\n",
       " 'RandomForest': {'mse': 0.007549035022716381, 'r2': 0.9929551143758598},\n",
       " 'Knn': {'mse': 0.10143649269856529, 'r2': 0.9053377700560483}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Model: linear\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X4linear_20191126-1926.sav\n",
      "El tiempo de seleccion fue 选择时间为: 9.105 s\n",
      "El error r2 de la familia linear es 错误分数: 0.896\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: gradient\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    9.0s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X4gradient_20191126-1926.sav\n",
      "El tiempo de seleccion fue 选择时间为: 9.666 s\n",
      "El error r2 de la familia gradient es 错误分数: 0.998\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: tree\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    6.3s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X4tree_20191126-1927.sav\n",
      "El tiempo de seleccion fue 选择时间为: 6.735 s\n",
      "El error r2 de la familia tree es 错误分数: 0.994\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: RandomForest\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   21.0s finished\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:356: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X4RandomForest_20191126-1927.sav\n",
      "El tiempo de seleccion fue 选择时间为: 21.862 s\n",
      "El error r2 de la familia RandomForest es 错误分数: 0.995\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "Model: Knn\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  45 | elapsed:    5.1s remaining:    0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at models/X4Knn_20191126-1927.sav\n",
      "El tiempo de seleccion fue 选择时间为: 6.138 s\n",
      "El error r2 de la familia Knn es 错误分数: 0.969\n",
      "********************************************************************************\n",
      "best model: gradient with an error r2 of: 0.9978801661896959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    5.7s finished\n"
     ]
    }
   ],
   "source": [
    "Bestmodels_X4, result_X4  = grid('X4', -1, X4_train, y_train.values, X4_test, y_test, models, score = 'r2', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear': {'mse': 0.07940088084831384, 'r2': 0.9259017712397531},\n",
       " 'gradient': {'mse': 0.004960807481584399, 'r2': 0.9953704915653491},\n",
       " 'tree': {'mse': 0.012129073955536895, 'r2': 0.9886809455133846},\n",
       " 'RandomForest': {'mse': 0.005216216165841511, 'r2': 0.9951321399134374},\n",
       " 'Knn': {'mse': 0.012137392325643556, 'r2': 0.9886731826714048}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestX0: {'RandomForest': 0.9841518217146826}\n",
      "BestX1: {'RandomForest': 0.9848360306165523}\n",
      "BestX2: {'RandomForest': 0.9844931372370535}\n",
      "BestX3: {'gradient': 0.9965349299301006}\n",
      "BestX4: {'gradient': 0.9953704915653491}\n"
     ]
    }
   ],
   "source": [
    "# 调用函数的方式  r2\n",
    "\n",
    "resultado = get_max(result_X0, 'r2')\n",
    "best_X0 = {}\n",
    "best_X0[resultado[0]] = resultado[1]\n",
    "print('BestX0: ' + str(best_X0))\n",
    "\n",
    "resultado = get_max(result_X1, 'r2')\n",
    "best_X1 = {}\n",
    "best_X1[resultado[0]] = resultado[1]\n",
    "print('BestX1: ' + str(best_X1))\n",
    "\n",
    "\n",
    "resultado = get_max(result_X2, 'r2')\n",
    "best_X2 = {}\n",
    "best_X2[resultado[0]] = resultado[1]\n",
    "print('BestX2: ' + str(best_X2))\n",
    "\n",
    "resultado = get_max(result_X3, 'r2')\n",
    "best_X3 = {}\n",
    "best_X3[resultado[0]] = resultado[1]\n",
    "print('BestX3: ' + str(best_X3))\n",
    "\n",
    "\n",
    "resultado = get_max(result_X4, 'r2')\n",
    "best_X4 = {}\n",
    "best_X4[resultado[0]] = resultado[1]\n",
    "print('BestX4: ' + str(best_X4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestX0: {'Knn': 0.15330473650832402}\n",
      "BestX1: {'RandomForest': 0.01624914044974528}\n",
      "BestX2: {'RandomForest': 0.016616572125573312}\n",
      "BestX3: {'gradient': 0.003713039034189975}\n",
      "BestX4: {'gradient': 0.004960807481584399}\n"
     ]
    }
   ],
   "source": [
    "# 调用函数的方式  均方误差\n",
    "\n",
    "resultado = get_max(result_X0, 'mse')\n",
    "best_X0 = {}\n",
    "best_X0[resultado[0]] = resultado[1]\n",
    "print('BestX0: ' + str(best_X0))\n",
    "\n",
    "resultado = get_min(result_X1, 'mse')\n",
    "best_X1 = {}\n",
    "best_X1[resultado[0]] = resultado[1]\n",
    "print('BestX1: ' + str(best_X1))\n",
    "\n",
    "\n",
    "resultado = get_min(result_X2, 'mse')\n",
    "best_X2 = {}\n",
    "best_X2[resultado[0]] = resultado[1]\n",
    "print('BestX2: ' + str(best_X2))\n",
    "\n",
    "resultado = get_min(result_X3, 'mse')\n",
    "best_X3 = {}\n",
    "best_X3[resultado[0]] = resultado[1]\n",
    "print('BestX3: ' + str(best_X3))\n",
    "\n",
    "\n",
    "resultado = get_min(result_X4, 'mse')\n",
    "best_X4 = {}\n",
    "best_X4[resultado[0]] = resultado[1]\n",
    "print('BestX4: ' + str(best_X4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mod': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False),\n",
       " 'par': {},\n",
       " 'bestModel': Pipeline(memory=None,\n",
       "          steps=[('scaler',\n",
       "                  StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                 ('linear',\n",
       "                  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                   normalize=False))],\n",
       "          verbose=False),\n",
       " 'r2': 0.8961022354901661,\n",
       " 'cols_order': array(['FlightDuration', 'PricePremium', 'PriceRelative'], dtype=object),\n",
       " 'selection_time': 9.105098247528076}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selected_model = Bestmodels_X4['linear']\n",
    "Bestmodels_X4['linear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(Selected_model, open('modeleconomy2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
